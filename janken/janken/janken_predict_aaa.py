"""
ã˜ã‚ƒã‚“ã‘ã‚“åˆ¤å®šAI - äºˆæ¸¬ãƒ»è©•ä¾¡ãƒ»ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»˜ããƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
ç”»åƒã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¦‹ã‚Œã‚‹Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™
"""

import os
import shutil
import tensorflow as tf
import numpy as np
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from janken_train_new import target_size
from janken_train_new import batch_size
from janken_train_new import preprocessing_function


def main():
    # è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    test_ds = tf.keras.utils.image_dataset_from_directory(
        "img_test",
        image_size=(target_size, target_size),
        batch_size=batch_size,
        label_mode="categorical",
        shuffle=False
    )
    
    # ã‚¯ãƒ©ã‚¹åã‚’å–å¾—
    class_names = test_ds.class_names
    print(f"ã‚¯ãƒ©ã‚¹å: {class_names}")
    
    # ã‚¯ãƒ©ã‚¹åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªï¼‰
    class_map = {}
    for i, name in enumerate(class_names):
        if 'gu' in name.lower():
            class_map[i] = 'ããƒ¼'
        elif 'tyoki' in name.lower() or 'choki' in name.lower():
            class_map[i] = 'ã¡ã‚‡ã'
        elif 'pa' in name.lower():
            class_map[i] = 'ã±ãƒ¼'
        else:
            class_map[i] = name
    
    # ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚·ãƒ³ã‚°ã‚’é©ç”¨
    test_ds_processed = test_ds.map(
        lambda x, y: (preprocessing_function(x), y),
        num_parallel_calls=tf.data.AUTOTUNE
    )
    test_ds_processed = test_ds_processed.prefetch(tf.data.AUTOTUNE)

    # æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
    true_labels = []
    for _, labels in test_ds:
        true_labels.extend(np.argmax(labels.numpy(), axis=1))
    true_labels = np.array(true_labels)
    
    # ç”»åƒãƒ‘ã‚¹ã‚’å–å¾—
    image_paths = test_ds.file_paths

    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆ.kerasã‚’å„ªå…ˆã€ãªã‘ã‚Œã°.h5ï¼‰
    if os.path.exists("model_with_subdirs.keras"):
        model = tf.keras.models.load_model("model_with_subdirs.keras")
        print("model_with_subdirs.kerasã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    elif os.path.exists("model.h5"):
        model = tf.keras.models.load_model("model.h5")
        print("model.h5ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        raise FileNotFoundError("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # äºˆæ¸¬å®Ÿæ–½
    pred_confidence = model.predict(test_ds_processed)
    pred_class = np.argmax(pred_confidence, axis=1)
    confidences = np.max(pred_confidence, axis=1)

    # äºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    print("\näºˆæ¸¬çµæœ:")
    print(pred_class)
    np.savetxt("result.csv", pred_class, fmt="%d")

    # ========== è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®— ==========
    print("\n" + "="*60)
    print("è©•ä¾¡çµæœ")
    print("="*60)
    
    # æ­£è§£ç‡
    accuracy = accuracy_score(true_labels, pred_class)
    print(f"\næ­£è§£ç‡ (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # é©åˆç‡ã€å†ç¾ç‡ã€Få€¤ï¼ˆãƒã‚¯ãƒ­å¹³å‡ã¨ã‚¯ãƒ©ã‚¹ã”ã¨ï¼‰
    precision, recall, f1, support = precision_recall_fscore_support(
        true_labels, pred_class, average=None, labels=[0, 1, 2]
    )
    
    print("\nã‚¯ãƒ©ã‚¹ã”ã¨ã®è©•ä¾¡æŒ‡æ¨™:")
    print("-" * 60)
    print(f"{'ã‚¯ãƒ©ã‚¹':<15} {'é©åˆç‡':<12} {'å†ç¾ç‡':<12} {'Få€¤':<12} {'ã‚µãƒ³ãƒ—ãƒ«æ•°':<12}")
    print("-" * 60)
    for i, class_name in enumerate(class_names):
        print(f"{class_name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<12}")
    
    # ãƒã‚¯ãƒ­å¹³å‡
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
        true_labels, pred_class, average='macro'
    )
    print("-" * 60)
    print(f"{'ãƒã‚¯ãƒ­å¹³å‡':<15} {precision_macro:<12.4f} {recall_macro:<12.4f} {f1_macro:<12.4f}")
    
    # é‡ã¿ä»˜ãå¹³å‡
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        true_labels, pred_class, average='weighted'
    )
    print(f"{'é‡ã¿ä»˜ãå¹³å‡':<15} {precision_weighted:<12.4f} {recall_weighted:<12.4f} {f1_weighted:<12.4f}")
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(true_labels, pred_class, labels=[0, 1, 2])
    print("\næ··åŒè¡Œåˆ—:")
    print("-" * 60)
    print(f"{'':>15}", end="")
    for class_name in class_names:
        print(f"{class_name:>15}", end="")
    print()
    for i, class_name in enumerate(class_names):
        print(f"{class_name:>15}", end="")
        for j in range(len(class_names)):
            print(f"{cm[i][j]:>15}", end="")
        print()
    
    # scikit-learnã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nè©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print("-" * 60)
    print(classification_report(true_labels, pred_class, target_names=class_names, digits=4))
    
    # è©•ä¾¡çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open("evaluation_result.txt", "w", encoding="utf-8") as f:
        f.write("="*60 + "\n")
        f.write("è©•ä¾¡çµæœ\n")
        f.write("="*60 + "\n\n")
        f.write(f"æ­£è§£ç‡ (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)\n\n")
        f.write("ã‚¯ãƒ©ã‚¹ã”ã¨ã®è©•ä¾¡æŒ‡æ¨™:\n")
        f.write("-" * 60 + "\n")
        f.write(f"{'ã‚¯ãƒ©ã‚¹':<15} {'é©åˆç‡':<12} {'å†ç¾ç‡':<12} {'Få€¤':<12} {'ã‚µãƒ³ãƒ—ãƒ«æ•°':<12}\n")
        f.write("-" * 60 + "\n")
        for i, class_name in enumerate(class_names):
            f.write(f"{class_name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<12}\n")
        f.write("-" * 60 + "\n")
        f.write(f"{'ãƒã‚¯ãƒ­å¹³å‡':<15} {precision_macro:<12.4f} {recall_macro:<12.4f} {f1_macro:<12.4f}\n")
        f.write(f"{'é‡ã¿ä»˜ãå¹³å‡':<15} {precision_weighted:<12.4f} {recall_weighted:<12.4f} {f1_weighted:<12.4f}\n\n")
        f.write("æ··åŒè¡Œåˆ—:\n")
        f.write("-" * 60 + "\n")
        f.write(classification_report(true_labels, pred_class, target_names=class_names, digits=4))
    
    print("\nè©•ä¾¡çµæœã‚’ evaluation_result.txt ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print("="*60)
    
    # ========== ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»˜ããƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ==========
    print("\nç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»˜ããƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    report_dir = "prediction_report"
    os.makedirs(report_dir, exist_ok=True)
    
    # ç”»åƒã‚’åˆ†é¡ã—ã¦ã‚³ãƒ”ãƒ¼
    failed_dir = os.path.join(report_dir, "failed_images")
    correct_dir = os.path.join(report_dir, "correct_images")
    os.makedirs(failed_dir, exist_ok=True)
    os.makedirs(correct_dir, exist_ok=True)
    
    # å¤±æ•—ã‚±ãƒ¼ã‚¹ã¨æˆåŠŸã‚±ãƒ¼ã‚¹ã®åé›†
    failed_predictions = []
    correct_predictions = []
    
    for i, (true_cls, pred_cls, conf, img_path) in enumerate(zip(true_labels, pred_class, confidences, image_paths)):
        img_info = {
            'index': i + 1,
            'filename': Path(img_path).name,
            'original_path': img_path,
            'true_class': class_map[true_cls],
            'pred_class': class_map[pred_cls],
            'true_class_id': true_cls,
            'pred_class_id': pred_cls,
            'confidence': conf,
            'is_correct': true_cls == pred_cls
        }
        
        if true_cls == pred_cls:
            correct_predictions.append(img_info)
        else:
            failed_predictions.append(img_info)
    
    # å¤±æ•—ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
    for failure in failed_predictions:
        src_path = failure['original_path']
        true_class = failure['true_class']
        pred_class = failure['pred_class']
        
        pattern_dir = os.path.join(failed_dir, f"å®Ÿéš›_{true_class}_äºˆæ¸¬_{pred_class}")
        os.makedirs(pattern_dir, exist_ok=True)
        
        filename = failure['filename']
        base_name = Path(filename).stem
        ext = Path(filename).suffix
        new_filename = f"{base_name}_conf{failure['confidence']:.3f}{ext}"
        
        dst_path = os.path.join(pattern_dir, new_filename)
        failure['report_path'] = os.path.relpath(dst_path, report_dir)
        
        if os.path.exists(src_path):
            shutil.copy2(src_path, dst_path)
    
    # æ­£è§£ç”»åƒã‚‚ã‚¯ãƒ©ã‚¹ã”ã¨ã«ã‚³ãƒ”ãƒ¼
    for correct in correct_predictions:
        src_path = correct['original_path']
        class_name = correct['true_class']
        
        class_dir = os.path.join(correct_dir, class_name)
        os.makedirs(class_dir, exist_ok=True)
        
        filename = correct['filename']
        base_name = Path(filename).stem
        ext = Path(filename).suffix
        new_filename = f"{base_name}_conf{correct['confidence']:.3f}{ext}"
        
        dst_path = os.path.join(class_dir, new_filename)
        correct['report_path'] = os.path.relpath(dst_path, report_dir)
        
        if os.path.exists(src_path):
            shutil.copy2(src_path, dst_path)
    
    # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    md_path = os.path.join(report_dir, "PREDICTION_REPORT.md")
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write("# ğŸ” ã˜ã‚ƒã‚“ã‘ã‚“åˆ¤å®šAI - äºˆæ¸¬çµæœãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        # ã‚µãƒãƒªãƒ¼
        f.write("## ğŸ“Š è©•ä¾¡ã‚µãƒãƒªãƒ¼\n\n")
        f.write(f"- **ç·åˆç²¾åº¦**: {accuracy*100:.2f}%\n")
        f.write(f"- **ãƒ†ã‚¹ãƒˆç·æ•°**: {len(true_labels)}æš\n")
        f.write(f"- **æ­£è§£**: {len(correct_predictions)}æš âœ…\n")
        f.write(f"- **ä¸æ­£è§£**: {len(failed_predictions)}æš âŒ\n")
        f.write(f"- **æˆåŠŸç‡**: {len(correct_predictions)/len(true_labels)*100:.2f}%\n\n")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
        f.write("### ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦\n\n")
        f.write("| ã‚¯ãƒ©ã‚¹ | Precision | Recall | F1-Score | ã‚µãƒ³ãƒ—ãƒ«æ•° |\n")
        f.write("|--------|-----------|--------|----------|------------|\n")
        for i, class_name in enumerate(class_names):
            display_name = class_map[i]
            f.write(f"| {display_name} | {precision[i]*100:.2f}% | {recall[i]*100:.2f}% | "
                   f"{f1[i]*100:.2f}% | {int(support[i])} |\n")
        f.write("\n---\n\n")
        
        # æ··åŒè¡Œåˆ—
        f.write("## ğŸ“ˆ æ··åŒè¡Œåˆ—\n\n")
        f.write("| å®Ÿéš›ï¼¼äºˆæ¸¬ |")
        for i in range(len(class_names)):
            f.write(f" {class_map[i]} |")
        f.write("\n|")
        f.write("------------|" * (len(class_names) + 1))
        f.write("\n")
        
        for i in range(len(class_names)):
            f.write(f"| **{class_map[i]}** |")
            for j in range(len(class_names)):
                count = cm[i, j]
                percent = (count / cm[i].sum() * 100) if cm[i].sum() > 0 else 0
                f.write(f" {count} ({percent:.1f}%) |")
            f.write("\n")
        
        f.write("\n---\n\n")
        
        # å¤±æ•—ã‚±ãƒ¼ã‚¹ï¼ˆç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»˜ãï¼‰
        if failed_predictions:
            f.write(f"## âŒ å¤±æ•—ã‚±ãƒ¼ã‚¹è©³ç´° ({len(failed_predictions)}ä»¶)\n\n")
            
            # èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«æ•´ç†
            failure_patterns = {}
            for failure in failed_predictions:
                pattern = f"{failure['true_class']} â†’ {failure['pred_class']}"
                if pattern not in failure_patterns:
                    failure_patterns[pattern] = []
                failure_patterns[pattern].append(failure)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚µãƒãƒªãƒ¼
            f.write("### èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚µãƒãƒªãƒ¼\n\n")
            f.write("| å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹ | äºˆæ¸¬ã‚¯ãƒ©ã‚¹ | ä»¶æ•° | å‰²åˆ |\n")
            f.write("|--------------|------------|------|------|\n")
            for pattern, pattern_failures in sorted(failure_patterns.items(), key=lambda x: len(x[1]), reverse=True):
                true_cls, pred_cls = pattern.split(' â†’ ')
                count = len(pattern_failures)
                percent = count / len(failed_predictions) * 100
                f.write(f"| {true_cls} | {pred_cls} | {count} | {percent:.1f}% |\n")
            f.write("\n")
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            for pattern, pattern_failures in sorted(failure_patterns.items()):
                true_cls, pred_cls = pattern.split(' â†’ ')
                f.write(f"### {pattern} ({len(pattern_failures)}ä»¶)\n\n")
                
                # ä¿¡é ¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
                sorted_failures = sorted(pattern_failures, key=lambda x: x['confidence'], reverse=True)
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã§ä¸€è¦§
                f.write("| # | ç”»åƒ | ãƒ•ã‚¡ã‚¤ãƒ«å | ä¿¡é ¼åº¦ |\n")
                f.write("|---|------|-----------|--------|\n")
                
                for i, failure in enumerate(sorted_failures, 1):
                    f.write(f"| {i} | ![]({failure['report_path']}) | `{failure['filename']}` | {failure['confidence']*100:.2f}% |\n")
                
                f.write("\n")
            
            f.write("---\n\n")
            
            # é«˜ä¿¡é ¼åº¦ã§ã®èª¤æ¤œå‡º
            f.write("## âš ï¸ é«˜ä¿¡é ¼åº¦ã§ã®èª¤æ¤œå‡º TOP 10\n\n")
            f.write("ãƒ¢ãƒ‡ãƒ«ãŒç¢ºä¿¡ã‚’æŒã£ã¦é–“é•ãˆãŸã‚±ãƒ¼ã‚¹ï¼ˆè¦æ³¨æ„ï¼‰:\n\n")
            
            sorted_all_failures = sorted(failed_predictions, key=lambda x: x['confidence'], reverse=True)[:10]
            
            f.write("| é †ä½ | ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ | ãƒ•ã‚¡ã‚¤ãƒ«å | å®Ÿéš› | äºˆæ¸¬ | ä¿¡é ¼åº¦ |\n")
            f.write("|------|---------------|-----------|------|------|--------|\n")
            
            for i, failure in enumerate(sorted_all_failures, 1):
                f.write(f"| {i} | ![]({failure['report_path']}) | `{failure['filename']}` | "
                       f"{failure['true_class']} | {failure['pred_class']} | {failure['confidence']*100:.2f}% |\n")
            
            f.write("\n")
        else:
            f.write("## ğŸ‰ å¤±æ•—ã‚±ãƒ¼ã‚¹ãªã—\n\n")
            f.write("å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§æ­£ã—ãäºˆæ¸¬ã•ã‚Œã¾ã—ãŸï¼\n\n")
        
        f.write("---\n\n")
        
        # æ­£è§£ã‚±ãƒ¼ã‚¹ï¼ˆç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»˜ãï¼‰
        if correct_predictions:
            f.write(f"## âœ… æ­£è§£ã‚±ãƒ¼ã‚¹ ({len(correct_predictions)}ä»¶)\n\n")
            
            # ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ•´ç†
            correct_by_class = {}
            for correct in correct_predictions:
                cls = correct['true_class']
                if cls not in correct_by_class:
                    correct_by_class[cls] = []
                correct_by_class[cls].append(correct)
            
            for cls, cls_correct in sorted(correct_by_class.items()):
                f.write(f"### {cls} ({len(cls_correct)}ä»¶)\n\n")
                
                # ä¿¡é ¼åº¦ã®é«˜ã„é †ã«è¡¨ç¤ºï¼ˆæœ€å¤§10ä»¶ï¼‰
                sorted_correct = sorted(cls_correct, key=lambda x: x['confidence'], reverse=True)[:10]
                
                f.write("| # | ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ | ãƒ•ã‚¡ã‚¤ãƒ«å | ä¿¡é ¼åº¦ |\n")
                f.write("|---|---------------|-----------|--------|\n")
                
                for i, correct in enumerate(sorted_correct, 1):
                    f.write(f"| {i} | ![]({correct['report_path']}) | `{correct['filename']}` | {correct['confidence']*100:.2f}% |\n")
                
                if len(cls_correct) > 10:
                    f.write(f"\n*ä»– {len(cls_correct) - 10}ä»¶ã¯çœç•¥*\n")
                
                f.write("\n")
        
        f.write("---\n\n")
        
        # æ”¹å–„ææ¡ˆ
        f.write("## ğŸ’¡ æ”¹å–„ææ¡ˆ\n\n")
        
        if failed_predictions:
            worst_class_idx = np.argmin(f1)
            worst_class = class_map[worst_class_idx]
            worst_f1 = f1[worst_class_idx] * 100
            
            f.write(f"1. **{worst_class}ã‚¯ãƒ©ã‚¹ã®ç²¾åº¦æ”¹å–„** (F1ã‚¹ã‚³ã‚¢: {worst_f1:.2f}%)\n")
            f.write(f"   - ã“ã®ã‚¯ãƒ©ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™\n")
            f.write(f"   - ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’å¼·åŒ–ã™ã‚‹\n\n")
            
            if failure_patterns:
                top_pattern = max(failure_patterns.items(), key=lambda x: len(x[1]))
                f.write(f"2. **{top_pattern[0]}ã®èª¤åˆ†é¡å¯¾ç­–** ({len(top_pattern[1])}ä»¶)\n")
                f.write(f"   - ã“ã®èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæœ€ã‚‚å¤šã„\n")
                f.write(f"   - ä¸Šè¨˜ã®ç”»åƒã‚’ç¢ºèªã—ã¦ã€å…±é€šã®ç‰¹å¾´ã‚’åˆ†æ\n\n")
            
            f.write("3. **é«˜ä¿¡é ¼åº¦ã§ã®èª¤æ¤œå‡ºç”»åƒã‚’ç¢ºèª**\n")
            f.write("   - ãƒ¢ãƒ‡ãƒ«ãŒç¢ºä¿¡ã‚’æŒã£ã¦é–“é•ãˆã¦ã„ã‚‹ç”»åƒã‚’é‡ç‚¹çš„ã«åˆ†æ\n")
            f.write("   - ãƒ©ãƒ™ãƒ«ãŒæ­£ã—ã„ã‹å†ç¢ºèª\n\n")
        
        f.write("---\n\n")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
        f.write("## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ\n\n")
        f.write("```\n")
        f.write("prediction_report/\n")
        f.write("â”œâ”€â”€ PREDICTION_REPORT.md (ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«)\n")
        f.write("â”œâ”€â”€ failed_images/\n")
        for pattern in sorted(failure_patterns.keys()):
            true_cls, pred_cls = pattern.split(' â†’ ')
            folder = f"å®Ÿéš›_{true_cls}_äºˆæ¸¬_{pred_cls}"
            count = len(failure_patterns[pattern])
            f.write(f"â”‚   â”œâ”€â”€ {folder}/ ({count}æš)\n")
        f.write("â””â”€â”€ correct_images/\n")
        for cls in sorted(correct_by_class.keys()):
            count = len(correct_by_class[cls])
            f.write(f"    â”œâ”€â”€ {cls}/ ({count}æš)\n")
        f.write("```\n\n")
        
        f.write("---\n\n")
        f.write("*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ `janken_predict_aaa.py` ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*\n")
    
    print(f"âœ“ ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»˜ããƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {md_path}")
    print(f"âœ“ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€: {report_dir}")
    print(f"  - å¤±æ•—ç”»åƒ: {len(failed_predictions)}æš")
    print(f"  - æ­£è§£ç”»åƒ: {len(correct_predictions)}æš")
    print("="*60)
    
    # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®è©³ç´°ãƒªã‚¹ãƒˆã‚‚ç”Ÿæˆ
    failed_list_path = 'failed_images_list.txt'
    with open(failed_list_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("é–“é•ãˆãŸç”»åƒä¸€è¦§\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n")
        f.write(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {len(true_labels)}æš\n")
        f.write(f"æ­£è§£æ•°: {len(correct_predictions)}æš ({len(correct_predictions)/len(true_labels)*100:.2f}%)\n")
        f.write(f"ä¸æ­£è§£æ•°: {len(failed_predictions)}æš ({len(failed_predictions)/len(true_labels)*100:.2f}%)\n")
        f.write("=" * 80 + "\n\n")
        
        if failed_predictions:
            failure_patterns = {}
            for failure in failed_predictions:
                pattern = f"{failure['true_class']} â†’ {failure['pred_class']}"
                if pattern not in failure_patterns:
                    failure_patterns[pattern] = []
                failure_patterns[pattern].append(failure)
            
            for pattern, pattern_failures in sorted(failure_patterns.items(), key=lambda x: len(x[1]), reverse=True):
                f.write(f"\nã€{pattern}ã€‘ {len(pattern_failures)}ä»¶\n")
                f.write("-" * 80 + "\n")
                
                sorted_failures = sorted(pattern_failures, key=lambda x: x['confidence'], reverse=True)
                
                for failure in sorted_failures:
                    f.write(f"  {failure['index']:3d}. {failure['filename']:<40} ")
                    f.write(f"å®Ÿéš›:{failure['true_class']:>6} â†’ äºˆæ¸¬:{failure['pred_class']:>6} ")
                    f.write(f"ä¿¡é ¼åº¦:{failure['confidence']*100:6.2f}%\n")
                    f.write(f"       ãƒ‘ã‚¹: {failure['original_path']}\n")
        else:
            f.write("\nğŸ‰ é–“é•ãˆãŸç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“ï¼å…¨ã¦æ­£è§£ã§ã™ï¼\n")
    
    print(f"âœ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®å¤±æ•—ãƒªã‚¹ãƒˆ: {failed_list_path}")
    print("="*60)


if __name__ == "__main__":
    main()
